- [Task 1: Configure Cosmos DB](#task-1-configure-cosmos-db)
- [Task 2: Configure Event Hubs](#task-2-configure-event-hubs)
- [Task 3: Configure Stream Analytics](#task-3-configure-stream-analytics)
- [Task 4: Configure Azure Function App](#task-4-configure-azure-function-app)
- [Task 5: Publish Function App and run data generator](#task-5-publish-function-app-and-run-data-generator)
- [Task 6: Create Power BI dashboard](#task-6-create-power-bi-dashboard)

## Task 1: Configure Cosmos DB

[Azure Cosmos DB](https://docs.microsoft.com/en-us/azure/cosmos-db/introduction) provides a multi-model, globally available NoSQL database with high concurrency, low latency, and predictable results. One of its biggest strengths is that it transparently synchronizes data to all regions around the globe, which can quickly and easily be added at any time. This adds value by reducing the amount of development required to read and write the data and removes any need for synchronization. The speed in which Cosmos DB can ingest as well as return data, coupled with its ability to do so at a global scale, makes it ideal for both ingesting real-time data and serving that data to consumers worldwide.

When storing and delivering your data on a global scale, there are some things to consider. Most distributed databases offer two consistency levels: strong and eventual. These live at different ends of a spectrum, where strong consistency often results in slower transactions because it synchronously writes data to each replica set. This guarantees that the reader will always see the most recent committed version of the data. Eventual consistency, on the other hand, asynchronously writes to each replica set with no ordering guarantee for reads. The replicas eventually converge, but the risk is that it can take several reads to retrieve the most up-to-date data.

Azure Cosmos DB was designed with control over the tradeoffs between read consistency, availability, latency, and throughput. This is why Cosmos DB offers five consistency levels: strong, bounded staleness, session, consistent prefix, and eventual. As a general rule of thumb, you can get about 2x read throughput for session, consistent prefix, and eventual consistency models compared to bounded staleness or strong consistency.

The Session consistency level is the default, and is suitable for most operations. It provides strong consistency for the session (application or connection), where all reads are current with writes from that session. Data from other sessions come in the correct order, but aren't guaranteed to be current. Session consistency level provides a balance of good performance and good availability at half the cost of both strong consistency and bounded staleness. As mentioned before, session provides about 2x read throughput compared to these two stronger consistency levels as well.

For this scenario, Contoso Auto does not need to ingest and serve their data globally just yet. Right now, they are working on a POC to rapidly ingest vehicle telemetry data, process that data as it arrives, and visualize the processed data through a real-time dashboard. Cosmos DB gives them the flexibility to add regions in the future either programmatically through its APIs, or through the "Replicate data globally" Cosmos DB settings in the portal.

To do this, go to the "Replicate data globally" settings, select the option to add a region, then choose the region you wish to add.

![The Add Region button is highlighted.](imgs/cosmos-add-region.png 'Configure regions')

Once you are finished adding regions, simply select the Save button to apply your changes. You will see the regions highlighted on a map.

![A map is displayed showing the regions that were added.](imgs/cosmos-region-map.png 'Regions')

To ensure high write and read availability, configure your Cosmos account to span at least two regions with multiple-write regions. This configuration will provide the availability, lowest latency, and scalability for both reads and writes backed by SLAs. To learn more, see [how to configure your Cosmos account with multiple write-regions](https://docs.microsoft.com/en-us/azure/cosmos-db/tutorial-global-distribution-sql-api). To configure multi-master in your applications, see [How to configure multi-master](https://docs.microsoft.com/en-us/azure/cosmos-db/how-to-multi-master).

You may be wondering how you can control the throughput, or speed at which data can be written to or read from Cosmos DB at a global level. In Azure Cosmos DB, provisioned throughput is represented as request units/second (RUs). RUs measure the cost of both read and write operations against your Cosmos DB container. Because Cosmos DB is designed with transparent horizontal scaling (e.g., scale out) and multi-master replication, you can very quickly and easily increase or decrease the number of RUs to handle thousands to hundreds of millions of requests per second around the globe with a single API call.

Cosmos DB allows you to increment/decrement the RUs in small increments of 1000 at the database level, and in even smaller increments of 100 RU/s at the container level. It is recommended that you configure throughput at the container granularity for guaranteed performance for the container all the time, backed by SLAs. Other guarantees that Cosmos DB delivers are 99.999% read and write availability all around the world, with those reads and writes being served in less than 10 milliseconds at the 99th percentile.

When you set a number of RUs for a container, Cosmos DB ensures that those RUs are available in all regions associated with your Cosmos DB account. When you scale out the number of regions by adding a new one, Cosmos will automatically provision the same quantity of RUs in the newly added region. You cannot selectively assign different RUs to a specific region. These RUs are provisioned for a container (or database) for all associated regions.

In this task, you will create a new Cosmos DB database and collection, set the throughput units, and obtain the connection details.

1.  To start, open a new web browser window and navigate to <https://portal.azure.com>. Log in with the credentials provided to you for this lab.

2.  After logging into the Azure portal, select **Resource groups** from the left-hand menu. Then select the resource group named **tech-immersion-YOUR_UNIQUE_IDENTIFIER**. The `YOUR_UNIQUE_IDENTIFIER` portion of the name is the unique identifier assigned to you for this lab.

    ![The tech-immersion resource group is selected.](imgs/tech-immersion-rg.png 'Resource groups')

3.  Select the **Azure Cosmos DB account** from the list of resources in your resource group.

    ![The Azure Cosmos DB account is selected in the resource group.](imgs/tech-immersion-rg-cosmos-db.png 'tech-immersion resource group')

4.  Within the Cosmos DB account blade, select **Data Explorer** on the left-hand menu.

    ![The Data Explorer link located in the left-hand menu is highlighted.](imgs/cosmos-db-data-explorer-link.png 'Data Explorer link')

5.  Select **New Collection** in the top toolbar.

    ![The New Collection link in the top toolbar is highlighted.](imgs/cosmos-db-new-collection-link.png 'New Collection link')

6.  In the **Add Collection** blade, configure the following:

    - **Database id:** Select **Create new**, then enter "ContosoAuto" for the id.
    - **Provision database throughput:** Unchecked.
    - **Collection id:** Enter "telemetry".
    - **Partition key:** Enter "/vin".
    - **Throughput:** Enter 15000.

    > The /vin partition was selected because the data will include this value, and it allows us to partition by vehicle from which the transaction originated. This field also contains a wide range of values, which is preferable for partitions.

    ![The Add Collection form is filled out with the previously mentioned settings entered into the appropriate fields.](imgs/cosmos-db-new-collection.png 'Add Collection')

    On the subject of partitions, choosing an appropriate partition key for Cosmos DB is a critical step for ensuring balanced reads and writes, scaling, and, in this case, in-order change feed processing per partition. While there are no limits, per se, on the number of logical partitions, a single logical partition is allowed an upper limit of 10 GB of storage. Logical partitions cannot be split across physical partitions. For the same reason, if the partition key chosen is of bad cardinality, you could potentially have skewed storage distribution. For instance, if one logical partition becomes larger faster than the others and hits the maximum limit of 10 GB, while the others are nearly empty, the physical partition housing the maxed out logical partition cannot split and could cause an application downtime. This is why we specified `vin` as the partition key. It has good cardinality for this data set.

7.  Select **OK** on the bottom of the form when you are finished entering the values.

8.  Select **Firewall and virtual networks** from the left-hand menu, then select Allow access from **All networks**. Select **Save**. This will allow the vehicle telemetry generator application to send data to your Cosmos DB collection. Select **Save**.

    ![The All networks option is selected within the Firewall and virtual networks blade.](imgs/cosmos-db-firewall.png 'Firewall and virtual networks')

9.  Select **Keys** from the left-hand menu.

    ![The Keys link on the left-hand menu is highlighted.](imgs/cosmos-db-keys-link.png 'Keys link')

10. Copy the **Primary Connection String** value by selecting the copy button to the right of the field. **SAVE THIS VALUE** in Notepad or similar text editor for later.

    ![The Primary Connection String key is copied.](imgs/cosmos-db-keys.png 'Keys')

## Task 2: Configure Event Hubs

Azure Event Hubs is a Big Data streaming platform and event ingestion service, capable of receiving and processing millions of events per second. We are using it to temporarily store vehicle telemetry data that is processed and ready to be sent to the real-time dashboard. As data flows into Event Hubs, Azure Stream Analytics will query the data, applying aggregates and tagging anomalies, then send it to Power BI.

In this task, you will create and configure a new event hub within the provided Event Hubs namespace. This will be used to capture vehicle telemetry after it has been processed and enriched by the Azure function you will create later on.

1.  Navigate to the [Azure portal](https://portal.azure.com).

2.  Select **Resource groups** from the left-hand menu. Then select the resource group named **tech-immersion-YOUR_UNIQUE_IDENTIFIER**.

    ![The tech-immersion resource group is selected.](imgs/tech-immersion-rg.png 'Resource groups')

3.  Select the **Event Hubs Namespace** from the list of resources in your resource group.

    ![The Event Hubs Namespace is selected in the resource group.](imgs/tech-immersion-rg-event-hubs.png 'tech-immersion resource group')

4.  Within the Event Hubs Namespace blade, select **Event Hubs** within the left-hand menu.

    ![The Event Hubs link is selected in the left-hand menu.](imgs/event-hubs-link.png 'Event Hubs link')

5.  Select **+ Event Hub** in the top toolbar to create a new event hub in the namespace.

    ![The new Event Hub link is highlighted in the top toolbar.](imgs/event-hubs-new-event-hub-link.png 'New event hub link')

6.  In the **Create Event Hub** blade, configure the following:

    - **Name:** Enter "telemetry".
    - **Partition Count:** Select 2.
    - **Message Retention**: Select 1.
    - **Capture:** Select Off.

    ![The Create Event Hub form is filled out with the previously mentioned settings entered into the appropriate fields.](imgs/event-hubs-create-event-hub.png 'Create Event Hub')

7.  Select **Create** on the bottom of the form when you are finished entering the values.

8.  Select your newly created **telemetry** event hub from the list after it is created.

    ![The newly created telemetry event hub is selected.](imgs/event-hubs-select.png 'Event hubs')

9.  Select **Shared access policies** from the left-hand menu.

    ![The Shared access policies link is selected in the left-hand menu.](imgs/event-hubs-shared-access-policies-link.png 'Shared access policies link')

10. Select **+ Add** in the top toolbar to create a new shared access policy.

    ![The Add button is highlighted.](imgs/event-hubs-shared-access-policies-add-link.png 'Add')

11. In the **Add SAS Policy** blade, configure the following:

    - **Name:** Enter "Read".
    - **Managed:** Unchecked.
    - **Send:** Unchecked.
    - **Listen:** Checked.

    ![The Add SAS Policy form is filled out with the previously mentioned settings entered into the appropriate fields.](imgs/event-hubs-add-sas-policy-read.png 'Add SAS Policy')

    > It is a best practice to create separate policies for reading, writing, and managing events. This follows the principle of least privilege to prevent services and applications from performing unauthorized operations.

12. Select **Create** on the bottom of the form when you are finished entering the values.

13. Select **+ Add** in the top toolbar to create a new shared access policy.

    ![The Add button is highlighted.](imgs/event-hubs-shared-access-policies-add-link.png 'Add')

14. In the **Add SAS Policy** blade, configure the following:

    - **Name:** Enter "Write".
    - **Managed:** Unchecked.
    - **Send:** Checked.
    - **Listen:** Unchecked.

    ![The Add SAS Policy form is filled out with the previously mentioned settings entered into the appropriate fields.](imgs/event-hubs-add-sas-policy-write.png 'Add SAS Policy')

15. Select **Create** on the bottom of the form when you are finished entering the values.

16. Select your **Read** policy from the list. Copy the **Connection string - primary key** value by selecting the Copy button to the right of the field. **SAVE THIS VALUE** in Notepad or similar text editor for later.

    ![The Read policy is selected and its blade displayed. The Copy button next to the Connection string - primary key field is highlighted.](imgs/event-hubs-read-policy-key.png 'SAS Policy: Read')

17. Now select your **Write** policy from the list. Copy the **Connection string - primary key** value by selecting the Copy button to the right of the field. **SAVE THIS VALUE** in Notepad or similar text editor for later.

    ![The Write policy is selected and its blade displayed. The Copy button next to the Connection string - primary key field is highlighted.](imgs/event-hubs-write-policy-key.png 'SAS Policy: Write')

## Task 3: Configure Stream Analytics

Azure Stream Analytics is an event-processing engine that allows you to examine high volumes of data streaming from devices. Incoming data can be from devices, sensors, web sites, social media feeds, applications, and more. It also supports extracting information from data streams, identifying patterns, and relationships. You can then use these patterns to trigger other actions downstream, such as create alerts, feed information to a reporting tool, or store it for later use.

In this task, you will configure Stream Analytics to use the event hub you created as a source, query and analyze that data, then send it to Power BI for reporting.

1.  Navigate to the [Azure portal](https://portal.azure.com).

2.  Select **Resource groups** from the left-hand menu. Then select the resource group named **tech-immersion-YOUR_UNIQUE_IDENTIFIER**.

    ![The tech-immersion resource group is selected.](imgs/tech-immersion-rg.png 'Resource groups')

3.  Select the **Stream Analytics job** from the list of resources in your resource group.

    ![The Stream Analytics job is selected in the resource group.](imgs/tech-immersion-rg-stream-analytics.png 'tech-immersion resource group')

4.  Within the Stream Analytics job blade, select **Inputs** within the left-hand menu.

    ![The Inputs link is selected in the left-hand menu.](imgs/inputs-link.png 'Inputs link')

5.  Select **+ Add stream input** in the top toolbar, then select **Event Hub** to create a new Event Hub input.

    ![The Add stream input button and Event Hub menu item are highlighted.](imgs/stream-analytics-add-input-link.png 'Add stream input - Event Hub')

6.  In the **New Input** blade, configure the following:

    - **Name:** Enter "eventhub".
    - **Select Event Hub from your subscriptions:** Selected.
    - **Subscription:** Make sure the subscription you are using for this lab is selected.
    - **Event Hub namespace:** Select the Event Hub namespace you are using for this lab.
    - **Event Hub name:** Select **Use existing**, then select **telemetry**, which you created earlier.
    - **Event Hub policy name:** Select **Read**.
    - Leave all other values at their defaults.

    ![The New Input form is filled out with the previously mentioned settings entered into the appropriate fields.](imgs/stream-analytics-new-input.png 'New Input')

7.  Select **Save** on the bottom of the form when you are finished entering the values.

8.  Within the Stream Analytics job blade, select **Outputs** within the left-hand menu.

    ![The Outputs link is selected in the left-hand menu.](imgs/outputs-link.png 'Outputs link')

9.  Select **+ Add** in the top toolbar, then select **Power BI** to create a new Power BI output.

    ![The Add button and Power BI menu item are highlighted.](imgs/stream-analytics-add-output-link.png 'Add output - Power BI')

10. In the **New Output** blade, select the **Authorize** button to authorize a connection from Stream Analytics to your Power BI account. If you do not have a Power BI account, select the **Sign up** link below the button.

    ![The Authorize button is highlighted in the New Output blade.](imgs/stream-analytics-new-output-authorize.png 'New Output')

11. When prompted, sign in to your Power BI account.

    ![The Power BI sign in form is displayed.](imgs/power-bi-sign-in.png 'Power BI Sign In')

12. After successfully signing in to your Power BI account, the New Output blade will update to show you are currently authorized.

    ![The New Output blade has been updated to show user is authorized to Power BI.](imgs/stream-analytics-new-output-authorized.png 'Authorized')

13. In the **New Output** blade, configure the following:

    - **Output alias:** Enter "powerBIAlerts".
    - **Group workspace:** Select My Workspace.
    - **Dataset name:** Enter "VehicleAnomalies".
    - **Table name:** Enter "Alerts".

    ![The New Output form is filled out with the previously mentioned settings entered into the appropriate fields.](imgs/stream-analytics-new-output.png 'New Output')

14. Select **Save** on the bottom of the form when you are finished entering the values.

15. Within the Stream Analytics job blade, select **Query** within the left-hand menu.

    ![The Query link is selected in the left-hand menu.](imgs/query-link.png 'Query link')

16. Clear the edit **Query** window and paste the following in its place:

    ```sql
    WITH
    Averages AS (
    select
        AVG(engineTemperature) averageEngineTemperature,
        AVG(speed) averageSpeed
    FROM
        eventhub TIMESTAMP BY [timestamp]
    GROUP BY
        TumblingWindow(Duration(second, 2))
    ),
    Anomalies AS (
    select
        t.vin,
        t.[timestamp],
        t.city,
        t.region,
        t.outsideTemperature,
        t.engineTemperature,
        a.averageEngineTemperature,
        t.speed,
        a.averageSpeed,
        t.fuel,
        t.engineoil,
        t.tirepressure,
        t.odometer,
        t.accelerator_pedal_position,
        t.parking_brake_status,
        t.headlamp_status,
        t.brake_pedal_status,
        t.transmission_gear_position,
        t.ignition_status,
        t.windshield_wiper_status,
        t.abs,
        (case when a.averageEngineTemperature >= 405 OR a.averageEngineTemperature <= 15 then 1 else 0 end) as enginetempanomaly,
        (case when t.engineoil <= 1 then 1 else 0 end) as oilanomaly,
        (case when (t.transmission_gear_position = 'first' OR
            t.transmission_gear_position = 'second' OR
            t.transmission_gear_position = 'third') AND
            t.brake_pedal_status = 1 AND
            t.accelerator_pedal_position >= 90 AND
            a.averageSpeed >= 55 then 1 else 0 end) as aggressivedriving
    from eventhub t TIMESTAMP BY [timestamp]
    INNER JOIN Averages a ON DATEDIFF(second, t, a) BETWEEN 0 And 2
    )
    SELECT
        *
    INTO
        powerBIAlerts
    FROM
        Anomalies
    where aggressivedriving = 1 OR enginetempanomaly = 1 OR oilanomaly = 1
    ```

    ![The query above has been inserted into the Query window.](imgs/stream-analytics-query.png 'Query window')

    The query averages the engine temperature and speed over a two second duration. Then it selects all telemetry data, including the average values from the previous step, and specifies the following anomalies as new fields:

    a. **enginetempanomaly**: When the average engine temperature is \>= 405 or \<= 15.

    b. **oilanomaly**: When the engine oil \<= 1.

    c. **aggressivedriving**: When the transmission gear position is in first, second, or third, and the brake pedal status is 1, the accelerator pedal position \>= 90, and the average speed is \>= 55.

    Finally, the query outputs all fields from the anomalies step into the `powerBIAlerts` output where aggressivedriving = 1 or enginetempanomaly = 1 or oilanomaly = 1.

17. Select **Save** in the top toolbar when you are finished updating the query.

18. Within the Stream Analytics job blade, select **Overview** within the left-hand menu. On top of the Overview blade, select **Start**.

    ![The Start button is highlighted on top of the Overview blade.](imgs/stream-analytics-overview-start-button.png 'Overview')

19. In the Start job blade that appears, select **Now** for the job output start time, then select **Start**. This will start the Stream Analytics job so it will be ready to start processing and sending your events to Power BI later on.

    ![The Now and Start buttons are highlighted within the Start job blade.](imgs/stream-analytics-start-job.png 'Start job')

## Task 4: Configure Azure Function App

Azure Functions is a solution for easily running small pieces of code, or "functions," in the cloud. You can write just the code you need for the problem at hand, without worrying about a whole application or the infrastructure to run it. Functions can make development even more productive, and you can use your development language of choice, such as C#, F#, Node.js, Java, or PHP.

When you use the Azure Functions consumption plan, you only pay for the time your code runs. Azure automatically handles scaling your functions to meet demand.

Azure Functions uses special bindings that allow you to automatically trigger the function when an event happens (a document is added to Azure Cosmos DB, a file is uploaded to blob storage, an event is added to Event Hubs, an HTTP request to the function is made, etc.), as well as to retrieve or send information to and from various Azure services. In the case of our function for this solution, we are using the `CosmosDBTrigger` to automatically trigger the function through the Cosmos DB change feed. This trigger supplies an input binding of type `IReadOnlyList<Document>` we name "input", that contains the records that triggered the function. This removes any code you would have to otherwise write to query that data. We also have an output binding to the event hub, of type `IAsyncCollector<EventData>`, which we name "eventHubOutput". Again, this reduces code by automatically sending data added to this collection to the specified event hub.

![The Cosmos DB trigger, input binding, and output binding are highlighted.](imgs/function-definition.png 'Azure function')

The function code itself is very lightweight, lending to the resource bindings and the `TelemetryProcessing.ProcessEvent` method that it calls:

![The function code is very lightweight.](imgs/function-code.png 'Function code')

The `TelemetryProcessing` class contains a simple method named `ProcessEvent` that evaluates the vehicle telemetry data sent by Cosmos DB and enriches it with the region name based on a simple map.

![Source code for the ProcessEvent method.](imgs/function-process-event.png 'TelemetryProcessing.ProcessEvent method')

In this task, you will configure the Function App with the Azure Cosmos DB and Event Hubs connection strings.

1.  Navigate to the [Azure portal](https://portal.azure.com).

2.  Select **Resource groups** from the left-hand menu. Then select the resource group named **tech-immersion-YOUR_UNIQUE_IDENTIFIER**.

    ![The tech-immersion resource group is selected.](imgs/tech-immersion-rg.png 'Resource groups')

3.  Select the **App Service** (Azure Function App) from the list of resources in your resource group.

    ![The App Service Function App is selected in the resource group.](imgs/tech-immersion-rg-function-app.png 'tech-immersion resource group')

4.  Within the Function App Overview blade, scroll down and select **Application settings**.

    ![The Function App Overview blade is displayed with the Application Settings link highlighted.](imgs/function-app-app-settings-link.png 'Function App overview')

5.  Select **Add new setting** at the bottom of the Application settings section.

    ![The Add new setting link is highlighted on the bottom of the Application settings section.](imgs/function-app-app-settings-new-link.png 'Application settings')

6.  Enter **CosmosDbConnectionString** into the **Name** field, then paste your Cosmos DB connection string into the **Value** field. If you cannot locate your connection string, refer to Task 1, step 10.

    ![The CosmosDbConnectionString name and value pair has been added and is highlighted.](imgs/function-app-app-settings-cosmos-db.png 'Application settings')

7.  Select **Add new setting** underneath the new application setting you just added to add a new one.

8.  Enter **EventHubsConnectionString** into the **Name** field, then paste your Event Hubs connection string into the **Value** field. If you cannot locate your connection string, refer to Task 2, step 17.

    ![The EventHubsConnectionString name and value pair has been added and is highlighted.](imgs/function-app-app-settings-event-hubs.png 'Application settings')

9.  Scroll to the top of the page and select **Save** in the top toolbar to apply your changes.

    ![The Save button is highlighted on top of the Application settings blade.](imgs/function-app-app-settings-save.png 'Application settings')

## Task 5: Publish Function App and run data generator

The data generator console application creates and sends simulated vehicle sensor telemetry for an array of vehicles (denoted by VIN (vehicle identification number)) directly to Cosmos DB. For this to happen, you first need to configure it with the Cosmos DB connection string.

In this task, you will open the lab solution in Visual Studio, publish the Function App, and configure and run the data generator. The data generator saves simulated vehicle telemetry data to Cosmos DB, which triggers the Azure function to run and process the data, sending it to Event Hubs, prompting your Stream Analytics job to aggregate and analyze the enriched data and send it to Power BI. The final step will be to create the Power BI report in the task that follows.

1.  Open Windows Explorer and navigate to `C:\lab-files`. Double-click on **TechImmersion.sln** to open the solution in Visual Studio. If you are prompted by Visual Studio to log in, log in with your Azure credentials you are using for this lab.

    ![The TechImmersion.sln file is highlighted in the C:\tech-immersion folder.](imgs/vs-solution.png 'Windows explorer')

    The Visual Studio solution contains the following projects:

    - **TechImmersion.CarEventProcessor**: Azure Function App project from which you will publish the Azure function that processes Cosmos DB documents as they arrive, and sends them to Event Hubs.
    - **TechImmersion.Common**: Common library that contains models and structs used by the other projects within the solution.
    - **TransactionGenerator**: Console app that generates simulated vehicle telemetry and writes it to Cosmos DB.

2.  Select the **Build** menu item, then select **Build Solution**. You should see a message in the output window on the bottom of the Visual Studio window that the build successfully completed. One of the operations that completes during this process is to download and install all NuGet packages.

    ![The Build menu item and Build Solution sub-menu item are highlighted.](imgs/vs-build-solution.png 'Build Solution')

3.  You will see the projects listed within the Solution Explorer in Visual Studio. Right-click the **TechImmersion.CarEventProcessor** solution, then select **Publish...** in the context menu.

    ![The TechImmersion.CarEventProcessor project and the Publish menu item are highlighted.](imgs/vs-publish-link.png 'Solution Explorer')

4.  Select **Select Existing** underneath Azure App Service since you will be publishing this to an existing Function App. Click **Publish** on the bottom of the dialog window. If you are prompted to log into your Azure Account, log in with the Azure account you are using for this lab.

    ![The Select Existing radio button and Publish button are highlighted.](imgs/vs-publish-target.png 'Pick a publish target')

5.  In the App Service dialog that follows, make sure your Azure **Subscription** for this lab is selected, then find and expand the **tech-immersion-YOUR_UNIQUE_IDENTIFIER** resource group. Select your Function App, then click **OK** on the bottom of the dialog window

    ![The Function App and OK button are highlighted.](imgs/vs-publish-app-service.png 'App Service')

6.  The Function App will start publishing in a moment. You can watch the output window for the publish status. When it is done publishing, you should see a "Publish completed" message on the bottom of the output window.

    ![The Publish Succeeded and Publish Completed messages are highlighted in the output window.](imgs/vs-publish-output.png 'Publish output')

7.  Expand the **TransactionGenerator** project within the Solution Explorer, then double-click on **appsettings.json** to open it.

    ![The appsettings.json file is highlighted in Solution Explorer.](imgs/vs-appsettings-link.png 'Solution Explorer')

8.  Paste your Cosmos DB connection string value next to `COSMOS_DB_CONNECTION_STRING`. Make sure you have quotes ("") around the value, as shown.

    ![The Cosmos DB connection string is highlighted within the appsettings.json file.](imgs/vs-appsettings.png 'appsettings.json')

    `SECONDS_TO_LEAD` is the amount of time to wait before sending vehicle telemetry data. Default value is `0`.

    `SECONDS_TO_RUN` is the maximum amount of time to allow the generator to run before stopping transmission of data. The default value is `600`. Data will also stop transmitting when you enter Ctrl+C while the generator is running, or if you close the window.

9.  Now you are ready to run the transaction generator. Select the **Debug** menu item, then select **Start Debugging**, or press _F-5_ on your keyboard.

    ![The Debug menu item and Start Debugging sub-menu item are selected](imgs/vs-debug.png 'Debug')

10. A new console window will open, and you should see it start to send data after a few seconds. Once you see that it is sending data to Cosmos DB, _minimize_ the window and keep it running in the background.

    ![Screenshot of the console window.](imgs/vs-console.png 'Console window')

    The top of the output displays information about the Cosmos DB collection you created (telemetry), the requested RU/s as well as estimated hourly and monthly cost. After every 1,000 records are requested to be sent, you will see output statistics.

---

Some key areas to point out about the data generator code are as follows:

Within the `Program.cs` file, we instantiate a new Cosmos DB client (`DocumentClient`), passing in the Cosmos DB service endpoint, authorization key, and connection policy (direct connect over TCP for fastest results). Next, we retrieve the Cosmos DB collection information and create an offer query (`CreateOfferQuery`) to pull statistics about the offered throughput in RU/s so we can estimate the monthly and hourly cost. Finally, we call the `SendData` method to start sending telemetry data to Cosmos DB.

![The telemetry generator code is displayed showing the Cosmos DB client instantiation.](imgs/telemetry-generator-code.png 'Telemetry generator code')

The `SendData` method outputs statistics about how much data was sent to Cosmos DB and how long it took to send, which varies based on your available system resources and internet bandwidth. It sends the telemetry data (`carEvent`) in one line of code:

```csharp
// Send to Cosmos DB:
var response = await _cosmosDbClient.CreateDocumentAsync(collectionUri, carEvent)
    .ConfigureAwait(false);
```

The last bit of interesting code within the generator is where we create the Cosmos DB database and collection if it does not exist. We also specify the collection partition key, indexing policy, and the throughput set to 15,000 RU/s:

![The InitializeCosmosDB method code.](imgs/telemetry-generator-initialize-cosmos.png 'InitializeCosmosDB method')

## Task 6: Create Power BI dashboard

In this task, you will use Power BI to create a report showing captured vehicle anomaly data. Then you will pin that report to a live dashboard for near real-time updates.

1.  Open your web browser and navigate to <https://powerbi.microsoft.com/>. Select **Sign in** on the upper-right.

    ![The Power BI home page is shown with the Sign in link highlighted.](imgs/pbi-signin.png 'Power BI home page')

2.  Enter your Power BI credentials you used when creating the Power BI output for Stream Analytics.

3.  After signing in, select **My Workspace** on the left-hand menu.

    ![The My Workspace link is selected on the left-hand menu.](imgs/pbi-my-workspace-link.png 'My Workspace')

4.  Select the **Datasets** tab on top of the workspace. Locate the dataset named **VehicleAnomalies**, then select the **Create Report** action button to the right of the name. If you do not see the dataset, you may need to wait a few minutes and refresh the page.

    ![The Datasets tab is selected in My Workspace and the VehicleAnomalies dataset is highlighted.](imgs/pbi-my-workspace.png 'Datasets')

5.  You should see a new blank report for VehicleAnomalies with the field list on the far right.

    ![A new blank report is displayed with the field list on the right.](imgs/pbi-blank-report.png 'Blank report')

6.  Select the **Map** visualization within the Visualizations section on the right.

    ![The Map visualization is highlighted.](imgs/pbi-map-vis.png 'Visualizations')

7.  Drag the **city** field to **Location**, and **aggressivedriving** to **Size**. This will place points of different sizes over cities on the map, depending on how many aggressive driving records there are.

    ![Screenshot displaying where to drag the fields onto the map settings.](imgs/pbi-map-fields.png 'Map settings')

8.  Your map should look similar to the following:

    ![The map is shown on the report.](imgs/pbi-map.png 'Map')

9.  Select a blank area on the report to deselect the map. Now select the **Treemap** visualization.

    ![The Treemap visualization is highlighted.](imgs/pbi-treemap-vis.png 'Visualization')

10. Drag the **enginetemperature** field to **Values**, then drag the **transmission_gear_position** field to **Group**. This will group the engine temperature values by the transmission gear position on the treemap so you can see which gears are associated with the hottest or coolest engine temperatures. The treemap sizes the groups according to the values, with the largest appearing on the upper-left and the lowest on the lower-right.


    ![Screenshot displaying where to drag the fields onto the treemap settings.](imgs/pbi-treemap-fields.png "Treemap settings")

11. Select the down arrow next to the **enginetemperature** field under **Values**. Select **Average** from the menu to aggregate the values by average instead of the sum.


    ![The Average menu option is highlighted for the enginetemperature value.](imgs/pbi-treemap-average.png "Average engine temperature")

12. Your treemap should look similar to the following:

    ![The treemap is shown on the report.](imgs/pbi-treemap.png 'Treemap')

13. Select a blank area on the report to deselect the treemap. Now select the **Area chart** visualization.

    ![The Area chart visualization is highlighted.](imgs/pbi-areachart-vis.png 'Area chart visualization')

14. Drag the **region** field to **Legend**, the **speed** field to **Values**, and the **timestamp** field to **Axis**. This will display an area chart with different colors indicating the region and the speed at which drivers travel over time within that region.

    ![Screenshot displaying where to drag the fields onto the area chart settings.](imgs/pbi-areachart-fields.png 'Area chart settings')

15. Select the down arrow next to the **speed** field under **Values**. Select **Average** from the menu to aggregate the values by average instead of the sum.

    ![The Average menu option is highlighted for the speed value.](imgs/pbi-areachart-average.png 'Average speed')

16. Your area chart should look similar to the following:

    ![The area chart on the report.](imgs/pbi-areachart.png 'Area chart')

17. Select a blank area on the report to deselect the area chart. Now select the **Multi-row card** visualization.

    ![The multi-card visualization is highlighted.](imgs/pbi-card-vis.png 'Multi-row card visualization')

18. Drag the **aggressivedriving** field, **enginetempanomaly**, and **oilanomaly** fields to **Fields**.

    ![Screenshot displaying where to drag the fields onto the multi-row card settings.](imgs/pbi-card-fields.png 'Multi-row card settings')

19. Select the **Format** tab in the multi-row card settings, then expand **Data labels**. Set the **Text size** to 30. Expand **Category labels** and set the **Text size** to 12.

    ![Screenshot of the format tab.](imgs/pbi-card-format.png 'Multi-row card format')

20. Your multi-row card should look similar to the following:

    ![The multi-row card on the report.](imgs/pbi-card.png 'Multi-row-card')

21. Select **Save** on the upper-right of the page.

    ![The save button is highlighted.](imgs/pbi-save.png 'Save')

22. Enter a name, such as "Vehicle Anomalies", then select **Save**.

    ![Screenshot of the save dialog.](imgs/pbi-save-dialog.png 'Save dialog')

23. Now let's add this report to a dashboard. Select **Pin Live Page** on the upper-right of the page.

    ![The Pin Live Page button is highlighted.](imgs/pbi-live.png 'Pin Live Page')

24. Select **New dashboard**, then enter a name, such as "Vehicle Anomalies Dashboard". Select **Pin live**. When prompted select the option to view the dashboard. Otherwise, you can find the dashboard under My Workspace on the left-hand menu.

    ![Screenshot of the pin to dashboard dialog.](imgs/pbi-live-dialog.png 'Pin to dashboard dialog')

25. The live dashboard will automatically refresh and update while data is being captured. You can hover over any point on a chart to view information about the item. Select one of the regions in the legend above the average speed chart. All other charts will filter by that region automatically. Click on a blank area of the chart to clear the filter.

    ![The live dashboard view.](imgs/pbi-dashboard.png 'Dashboard')

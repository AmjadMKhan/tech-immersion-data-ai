<h1 id="data-ai-tech-immersion-workshop-product-review-guide-and-lab-instructions">Data &amp; AI Tech Immersion Workshop – Product Review Guide and Lab Instructions</h1>
<h2 id="day-1-experience-1---handling-big-data-with-sql-server-2019-big-data-clusters">Day 1, Experience 1 - Handling Big Data with SQL Server 2019 Big Data Clusters</h2>
<ul>
<li><a href="#data--ai-tech-immersion-workshop-%E2%80%93-product-review-guide-and-lab-instructions">Data &amp; AI Tech Immersion Workshop – Product Review Guide and Lab Instructions</a>
<ul>
<li><a href="#day-1-experience-1---handling-big-data-with-sql-server-2019-big-data-clusters">Day 1, Experience 1 - Handling Big Data with SQL Server 2019 Big Data Clusters</a></li>
<li><a href="#technology-overview">Technology overview</a></li>
<li><a href="#scenario-overview">Scenario overview</a></li>
<li><a href="#experience-requirements">Experience requirements</a></li>
<li><a href="#before-the-lab-connecting-to-sql-server-2019">Before the lab: Connecting to SQL Server 2019</a>
<ul>
<li><a href="#connect-with-azure-data-studio">Connect with Azure Data Studio</a></li>
<li><a href="#connect-with-sql-server-management-studio">Connect with SQL Server Management Studio</a></li>
</ul></li>
<li><a href="#task-1-query-and-join-data-from-flat-files-data-from-external-database-systems-and-sql-server">Task 1: Query and join data from flat files, data from external database systems, and SQL Server</a></li>
<li><a href="#task-2-train-a-machine-learning-model-score-and-save-data-as-external-table">Task 2: Train a machine learning model, score and save data as external table</a></li>
<li><a href="#task-3-identify-pii-and-gdpr-related-compliance-issues-using-data-discovery--classification-in-ssms">Task 3: Identify PII and GDPR-related compliance issues using Data Discovery &amp; Classification in SSMS</a></li>
<li><a href="#task-4-fix-compliance-issues-with-dynamic-data-masking">Task 4: Fix compliance issues with dynamic data masking</a></li>
<li><a href="#wrap-up">Wrap-up</a></li>
<li><a href="#additional-resources-and-more-information">Additional resources and more information</a></li>
</ul></li>
</ul>
<h2 id="technology-overview">Technology overview</h2>
<p>SQL Server 2019 brings innovative security and compliance features, industry leading performance, mission-critical availability, and advanced analytics to all data workloads, now with support for big data built-in.</p>
<p>SQL Server 2019 is a hub for data integration. Data virtualization allows queries across relational and non-relational data without movement or replication. The enhanced PolyBase feature of SQL Server 2019 is able to connect to Hadoop clusters, Oracle, Teradata, MongoDB, and more.</p>
<p>Customers will be able to deliver transformational insights over structured and unstructured data with the power of SQL Server, Hadoop and Spark. SQL Server 2019 big data clusters offer scalable compute and storage composed of SQL Server, Spark and HDFS. Big data clusters will also cache data in scale-out data marts.</p>
<p>SQL Server 2019 is a complete AI platform to train and operationalize R and Python models in SQL Server Machine Learning Services or Spark ML using Azure Data Studio notebooks.</p>
<p>SQL Server 2019 will give customers and ISVs the choice of programming language and platform. They will be able to build modern applications with innovative features using .NET, PHP, Node.JS, Java, Python, Ruby, and more – and deploy the application on either Windows, Linux, or containers both on-premises and in the cloud. Application developers are now able to run Java code on SQL Server and store and analyze graph data.</p>
<p>SQL Server 2019 allows customers to run real-time analytics on operational data using HTAP (Hybrid Transactional and Analytical Processing), leverage the in-memory technologies for faster transactions and analytical queries, and get higher concurrency and scale through persistent memory.</p>
<p>Intelligent Query Processing features in SQL Server 2019 improve scaling of queries, and Automatic Plan Correction resolves performance problems.</p>
<p>SQL Server 2019 enables several layers of security including protection of computations in Always Encrypted secure enclaves. Customers can track compliance with sophisticated tools such as Data Discovery &amp; Classification labeling for GDPR and Vulnerability Assessment tool.</p>
<p>For High Availability and Disaster Recovery, SQL Server 2019 now supports up to eight secondary replicas in an Always On Availability Group. Customers can also run Always On Availability Groups on containers using Kubernetes.</p>
<p>SQL Server 2019 also has powerful tools for Business Intelligence including Analysis Services and Power BI Report Server which provide visual data exploration and interactive analysis of business data.</p>
<h2 id="scenario-overview">Scenario overview</h2>
<p>Contoso Auto stores data in several data stores, including relational databases, NoSQL databases, data warehouses, and unstructured data stored in a data lake. They have heard of data virtualization in SQL Server 2019, and are interested to see whether this feature will allow them to more easily access their data stored in these disparate locations. They have heard of the new Big Data Clusters that can be scaled out to handle their Big Data workloads, including machine learning tasks and advanced analytics. They are also interested in any performance improvements against their internal SQL tables by moving to 2019, since the overall amount of data is growing at a rapid pace.</p>
<p>This experience will highlight the new features of SQL Server 2019 with a focus on Big Data Clusters and data virtualization. You will gain hands-on experience with querying both structured and unstructured data in a unified way using T-SQL. This capability will be illustrated by joining different data sets, such as product stock data in flat CSV files in Azure Storage, product reviews stored in Azure SQL Database, and transactional data in SQL Server 2019 for exploratory data analysis within Azure Data Studio. This joined data will be prepared into a table used for reporting, highlighting query performance against this table due to intelligent query processing. With the inclusion of Apache Spark packaged with Big Data Clusters, it is now possible to use Spark to train machine learning models over data lakes and use those models in SQL Server in one system. You will learn how to use Azure Data Studio to work with Jupyter notebooks to train a simple model that can predict vehicle battery lifetime, score new data and save the result as an external table. Finally, you will experience the data security and compliance features provided by SQL Server 2019 by using the Data Discovery &amp; Classification tool in SSMS to identify tables and columns with PII and GDPR-related compliance issues, then address the issues by layering on dynamic data masking to identified columns.</p>
<h2 id="experience-requirements">Experience requirements</h2>
<p>Before you begin this lab, you need to find the following information on the Tech Immersion Mega Data &amp; AI Workshop On Demand Lab environment details page, or the document provided to you for this experience:</p>
<ul>
<li>SQL Server 2019 Big Data Cluster IP address and port number: <code>SQL SERVER_2019_CLUSTER URL</code></li>
<li>SQL username: <code>SQL 2019 Big Data Cluster username</code></li>
<li>SQL password: <code>SQL 2019 Big Data Cluster password</code></li>
<li>Sales database name (your unique copy): <code>SALES DB</code></li>
<li>Azure SQL Database server: <code>AZURE DATABASE SERVER</code></li>
<li>Azure SQL Database name: <code>DATABASE NAME</code></li>
<li>Azure SQL Database username: <code>DATABASE USER</code></li>
<li>Azure SQL Database password: <code>DATABASE PASSWORD</code></li>
</ul>
<h2 id="before-the-lab-connecting-to-sql-server-2019">Before the lab: Connecting to SQL Server 2019</h2>
<p>Follow the steps below to connect to your SQL Server 2019 cluster with both Azure Data Studio and SQL Server Management Studio (SSMS).</p>
<h3 id="connect-with-azure-data-studio">Connect with Azure Data Studio</h3>
<p>A link to Azure Data Studio should already be on the desktop of the VM. If not, follow the instructions in Step 1 below.</p>
<figure>
<img src="media/ads-desktop.png" title="Desktop" alt="Azure Data Studio is highlighted on the desktop." /><figcaption>Azure Data Studio is highlighted on the desktop.</figcaption>
</figure>
<ol type="1">
<li><p>On the bottom-left corner of your Windows desktop, locate the search box next to the Start Menu. Type <strong>Azure Data Studio</strong>, then select the Azure Data Studio desktop app in the search results.</p>
<figure>
<img src="media/launch-azure-data-studio.png" title="Launch Azure Data Studio" alt="The search box has “Azure Data Studio” entered into it and the desktop app is highlighted in the results." /><figcaption>The search box has “Azure Data Studio” entered into it and the desktop app is highlighted in the results.</figcaption>
</figure>
<blockquote>
<p><strong>Please note:</strong> If Azure Data Studio prompts you to update, please <strong>do not apply</strong> the update at this time. The lab has been tested with the software and library versions loaded in the provided environment.</p>
</blockquote></li>
<li><p>Within Azure Data Studio, select <strong>Servers</strong> from the top of the left-hand menu, then select <strong>New Connection</strong> from the top toolbar to the right of the menu.</p>
<figure>
<img src="media/ads-new-connection-link.png" title="Azure Data Studio" alt="The Servers menu icon is selected, as well as the new connection icon." /><figcaption>The Servers menu icon is selected, as well as the new connection icon.</figcaption>
</figure></li>
<li><p>Within the Connection dialog, configure the following:</p>
<ul>
<li><strong>Connection type:</strong> Select Microsoft SQL Server.</li>
<li><strong>Server:</strong> Enter the IP address, followed by port number <code>31433</code> to the SQL Server 2019 Big Data cluster. Use the value from the <code>SQL SERVER_2019_CLUSTER URL</code> for this from the environment documentation. It should have a format of IP separated by a comma from the port, such as: <code>11.122.133.144,31433</code>.</li>
<li><strong>Authentication type:</strong> Select SQL Login.</li>
<li><strong>Username:</strong> Enter <code>sa</code>.</li>
<li><strong>Password:</strong> Enter the password provided to you for this lab, you can find this value documented as <code>SQL 2019 Big Data Cluster password</code>.</li>
<li><strong>Remember password:</strong> Checked.</li>
<li>Leave all other options at their default values.</li>
</ul>
<figure>
<img src="media/ads-new-connection.png" title="Azure Data Studio - New Connection" alt="The Connection form is filled out with the previously mentioned settings entered into the appropriate fields." /><figcaption>The Connection form is filled out with the previously mentioned settings entered into the appropriate fields.</figcaption>
</figure></li>
<li><p>Click <strong>Connect</strong>.</p></li>
</ol>
<h3 id="connect-with-sql-server-management-studio">Connect with SQL Server Management Studio</h3>
<p>The version of SQL Server Management Studio (SSMS) used in this lab is v17.x. There is a <a href="https://docs.microsoft.com/en-us/sql/ssms/download-sql-server-management-studio-ssms?view=sql-server-2017#ssms-180-preview-7">newer preview version (v18)</a> that includes some SQL Server 2019 features, which is not included in the provided environment.</p>
<ol type="1">
<li><p>On the bottom-left corner of your Windows desktop, locate the search box next to the Start Menu. Type <strong>SQL Server Management Studio</strong>, then select the SQL Server Management Studio desktop app in the search results.</p>
<figure>
<img src="media/launch-ssms.png" title="Launch SQL Server Management Studio" alt="The search box has “SQL Server Management Studio” entered into it and the desktop app is highlighted in the results." /><figcaption>The search box has “SQL Server Management Studio” entered into it and the desktop app is highlighted in the results.</figcaption>
</figure></li>
<li><p>Within the Connection dialog that appears, configure the following:</p>
<ul>
<li><strong>Server name:</strong> Enter the IP address, followed by port number <code>31433</code> to the SQL Server 2019 Big Data cluster. Use the value from the <code>SQL SERVER_2019_CLUSTER URL</code> for this from the environment documentation. It should have a format of IP separated by a comma from the port, such as: <code>11.122.133.144,31433</code>.</li>
<li><strong>Authentication:</strong> Select SQL Server Authentication.</li>
<li><strong>Login:</strong> Enter <code>sa</code>.</li>
<li><strong>Password:</strong> Enter the password provided to you for this lab, you can find this value documented as <code>SQL 2019 Big Data Cluster password</code>.</li>
<li><strong>Remember password:</strong> Checked.</li>
</ul>
<figure>
<img src="media/ssms-connection.png" title="SQL Server Management Studio - Connect" alt="The Connect form is filled out with the previously mentioned settings entered into the appropriate fields." /><figcaption>The Connect form is filled out with the previously mentioned settings entered into the appropriate fields.</figcaption>
</figure></li>
<li><p>Click <strong>Options &gt;&gt;</strong>.</p></li>
<li><p>Select the <strong>Additional Connection Parameters</strong> tab. In the text area below, enter <code>TrustServerCertificate=True</code>. This is needed because the server certificates are dynamically generated for the Big Data Clusters, and are self-signed.</p>
<figure>
<img src="media/ssms-connection-additional.png" title="Additional Connection Parameters" alt="The Additional Connection Parameters tab is selected and the TrustServerCertificate=True value is highlighted." /><figcaption>The Additional Connection Parameters tab is selected and the TrustServerCertificate=True value is highlighted.</figcaption>
</figure></li>
<li><p>Click <strong>Connect</strong>.</p></li>
</ol>
<h2 id="task-1-query-and-join-data-from-flat-files-data-from-external-database-systems-and-sql-server">Task 1: Query and join data from flat files, data from external database systems, and SQL Server</h2>
<p>One of the key new features of SQL Server 2019 is data virtualization. What this means is that you can <em>virtualize</em> external data in a SQL Server instance, regardless of source, location, and format, so that it can be queried like any other table, or sets of tables, within your SQL Server instance. In essence, data virtualization helps you create a single “virtual” layer of data from these disparate sources, providing unified data services to support multiple applications and users. A more familiar term we could use is data lake, or perhaps data hub. Unlike a typical data lake, however, you do not have to move data out from where it lives, yet you can still query that data through a consistent interface. This is a huge advantage over traditional ETL (extract-transform-load) processes where data must be moved from its original source to a new destination, oftentimes with some data transformation or mapping. This causes delays, extra storage, additional security, and a fair amount of engineering in most cases. With data virtualization, no data movement is required, which means the data sets are up-to-date, and it is possible to query and join these different data sources through these new capabilities, thanks to the use of new <a href="https://docs.microsoft.com/sql/relational-databases/polybase/polybase-guide?view=sql-server-ver15">PolyBase</a> connectors. The data sources you can connect to include Cosmos DB, SQL Server (including Azure SQL Database), Oracle, HDFS (for flat files), and DB2.</p>
<figure>
<img src="media/data-virtualization-vs-etl.png" title="ETL vs. Data Virtualization" alt="Data Virtualization compared to traditional ETL." /><figcaption>Data Virtualization compared to traditional ETL.</figcaption>
</figure>
<p>The image to the left represents traditional data movement using ETL. Compare that to data virtualization, which does not require data movement and provides a unified layer over top of existing data sources.</p>
<p>In this task, you will experience how to configure data virtualization in SQL Server 2019 by joining data in a single query from a SQL Server 2019 table, an external file stored in HDFS (Hadoop Filesystem), and an external database.</p>
<p>Learn more about using data virtualization with <a href="https://docs.microsoft.com/sql/relational-databases/polybase/data-virtualization?toc=%2fsql%2fbig-data-cluster%2ftoc.json&amp;view=sql-server-ver15">relational data sources</a> and <a href="https://docs.microsoft.com/sql/relational-databases/polybase/data-virtualization-csv?view=sql-server-ver15">CSV files</a>, using the External Table Wizard.</p>
<p>To start, we will use the External Table Wizard in Azure Data Studio to connect to an external Azure SQL Database.</p>
<ol type="1">
<li><p>Open Azure Data Studio and connect to your SQL Server 2019 cluster, following the <a href="#connect-with-azure-data-studio">connection steps</a> above.</p></li>
<li><p>Expand the Databases folder, right-click on the <strong>sales_YOUR_UNIQUE_IDENTIFIER</strong> database, then select <strong>Create External Table</strong>. The <code>YOUR_UNIQUE_IDENTIFIER</code> portion of the name is the unique identifier assigned to you for this lab.</p>
<figure>
<img src="media/ads-create-external-table-sales.png" title="Create External Table" alt="The sales database and the Create External Table sub-menu item are highlighted." /><figcaption>The sales database and the Create External Table sub-menu item are highlighted.</figcaption>
</figure></li>
<li><p>Select the <strong>SQL Server</strong> data source type, then click <strong>Next</strong>.</p>
<figure>
<img src="media/ads-external-table-wizard-data-source-type.png" title="Select data source type" alt="The SQL Server data source type is selected." /><figcaption>The SQL Server data source type is selected.</figcaption>
</figure></li>
<li><p>The next step is to create a database master key, if it does not already exist. This secures the credentials used by an external data source. Enter <code>MySecure@MasterKey1</code> in the <strong>Password</strong> and <strong>Confirm Password</strong> fields. If you see a message stating that a master key already exists, you may skip this step. Click <strong>Next</strong>.</p>
<figure>
<img src="media/ads-external-table-wizard-master-key.png" title="Master Key" alt="The Master Key step is displayed." /><figcaption>The Master Key step is displayed.</figcaption>
</figure></li>
<li><p>Now, enter the credentials provided to you for the <strong>CA_Commerce</strong> Azure SQL Database within the following fields:</p>
<ul>
<li><strong>External Data Source Name:</strong> Enter the string “SQLReviews”.</li>
<li><strong>Server Name:</strong> Enter the value provided to you for the Azure SQL Server name. The name should end with <code>.database.windows.net</code> (you can find this value in the lab environment details page, with the label <code>AZURE DATABASE SERVER</code>).</li>
<li><strong>Database Name:</strong> Enter “CA_Commerce”.</li>
<li><strong>Choose Credential:</strong> Select “– Create New Credential –”.</li>
<li><strong>New Credential Name:</strong> Enter “SQLCred”.</li>
<li><strong>Username:</strong> Enter the Azure SQL Server username provided to you for this lab (look to the lab environment details for the label <code>DATABASE USER</code>).</li>
<li><strong>Password:</strong> Enter the Azure SQL Server password provided to you for this lab (look to the lab environment details for the label <code>DATABASE PASSWORD</code>).</li>
</ul>
<figure>
<img src="media/ads-external-table-wizard-data-source.png" title="Create a connection to your Data Source" alt="The external data source connection form is filled out with the previously mentioned settings entered into the appropriate fields." /><figcaption>The external data source connection form is filled out with the previously mentioned settings entered into the appropriate fields.</figcaption>
</figure></li>
<li><p>Click <strong>Next</strong>. This process will take a few moments while the External Table Wizard attempts to connect to your data source.</p></li>
<li><p>The next screen allows you to configure external table mapping and select the tables for which you want to create external views. Expand the <strong>CA_Commerce</strong> database node, then expand Tables, and check the box next to the <strong>dbo.Reviews</strong> table. Click on the table name to highlight it as well. It is here where you can rename the external table if you wish. For now, just click <strong>Next</strong>.</p>
<figure>
<img src="media/ads-external-table-wizard-table-mapping.png" title="External tables" alt="The external tables are listed and the Reviews table is checked and selected." /><figcaption>The external tables are listed and the Reviews table is checked and selected.</figcaption>
</figure></li>
<li><p>In the Summary page that follows, you can see the name of the database scoped credential and external data source objects to be created in the destination database. Here you can click Generate Script to view the SQL script that will run to create the external table. Instead, click <strong>Create</strong>.</p>
<figure>
<img src="media/ads-external-table-wizard-summary.png" title="Summary" alt="A screenshot of the summary is displayed." /><figcaption>A screenshot of the summary is displayed.</figcaption>
</figure></li>
<li><p>After a few moments, a “Create External Table succeeded” message will display.</p>
<figure>
<img src="media/ads-external-table-wizard-succeeded.png" title="Create External Table succeeded message" alt="The Create External Table succeeded message is displayed." /><figcaption>The Create External Table succeeded message is displayed.</figcaption>
</figure></li>
<li><p>Select the Servers link (Ctrl+G) on the left-hand menu, then expand the Tables list underneath your <strong>sales_YOUR_UNIQUE_IDENTIFIER</strong> database and find the <strong>dbo.Reviews (External)</strong> table. If you do not see it, right-click on the Tables folder, then select Refresh. The “(External)” portion of the table name denotes that it is a virtual data object that was added as an external table.</p>
<figure>
<img src="media/ads-reviews-table-in-list.png" title="Reviews external table" alt="The Reviews external table is displayed in the sales tables list." /><figcaption>The Reviews external table is displayed in the sales tables list.</figcaption>
</figure></li>
<li><p>Right-click the <strong>dbo.Reviews (External)</strong> table, then select the <strong>Select Top 1000</strong> menu option to display the table contents.</p>
<figure>
<img src="media/ads-reviews-select-top-1000.png" title="Select Top 1000" alt="The Select Top 1000 rows menu item is highlighted." /><figcaption>The Select Top 1000 rows menu item is highlighted.</figcaption>
</figure></li>
<li><p>You should see a SQL query selecting the top 1000 records from the Reviews table and its results. The interesting thing to note is that the query selects the table and fields using the same syntax you would use to select from any other table in the sales database. The fact that the Reviews table is external is completely seamless and transparent to the user. This is the power of data virtualization in SQL Server 2019.</p>
<figure>
<img src="media/ads-reviews-query-results.png" title="Reviews query results" alt="The Reviews query and results are displayed." /><figcaption>The Reviews query and results are displayed.</figcaption>
</figure>
<div class="sourceCode" id="cb1"><pre class="sourceCode sql"><code class="sourceCode sql"><a class="sourceLine" id="cb1-1" title="1"><span class="kw">SELECT</span> TOP (<span class="dv">1000</span>) [product_id]</a>
<a class="sourceLine" id="cb1-2" title="2">    ,[customer_id]</a>
<a class="sourceLine" id="cb1-3" title="3">    ,[review]</a>
<a class="sourceLine" id="cb1-4" title="4">    ,[date_added]</a>
<a class="sourceLine" id="cb1-5" title="5"><span class="kw">FROM</span> [sales_YOUR_UNIQUE_IDENTIFIER].[dbo].[Reviews]</a></code></pre></div></li>
<li><p>The next data source we will be virtualizing is a CSV file that was uploaded to HDFS.</p></li>
<li><p>Within Azure Data Studio, scroll down below the list of SQL Server 2019 databases to find the <strong>Data Services</strong> folder. Expand that folder, expand the <strong>HDFS</strong> folder, then expand the <strong>data</strong> subfolder. Right-click on the <code>stockitemholdings.csv</code> file, then select <strong>Create External Table From CSV Files</strong>.</p>
<figure>
<img src="media/ads-create-external-table-csv.png" title="Create External Table From CSV Files" alt="The CSV file and the Create External Table From CSV Files menu item are highlighted." /><figcaption>The CSV file and the Create External Table From CSV Files menu item are highlighted.</figcaption>
</figure></li>
<li><p>The first dialog has you select the SQL Server Master instance containing your Big Data Cluster. Select the connection underneath <strong>Active SQL Server connections</strong> that includes the cluster’s IP address and the <strong>sales_YOUR_UNIQUE_IDENTIFIER</strong> database name.</p>
<figure>
<img src="media/ads-external-table-csv-wizard-active-connection.png" title="Active SQL Server connections" alt="The active SQL Server connection is highlighted." /><figcaption>The active SQL Server connection is highlighted.</figcaption>
</figure></li>
<li><p>Click <strong>Next</strong>.</p></li>
<li><p>In the destination database step, select the <strong>sales_YOUR_UNIQUE_IDENTIFIER</strong> database underneath <strong>Database the external table will be created in</strong>. Leave the name and schema at their defaults, then click <strong>Next</strong>.</p>
<figure>
<img src="media/ads-external-table-csv-destination.png" title="Destination database" alt="The sales database is selected and highlighted." /><figcaption>The sales database is selected and highlighted.</figcaption>
</figure></li>
<li><p>The next step displays a preview of the first 50 rows CSV data for validation. Click <strong>Next</strong> to continue.</p>
<figure>
<img src="media/ads-external-table-csv-preview.png" title="Preview Data" alt="A preview of the CSV data is displayed." /><figcaption>A preview of the CSV data is displayed.</figcaption>
</figure></li>
<li><p>In the next step, you will be able to modify the columns of the external table you intend to create. You are able to alter the column name, change the data type, and allow for Nullable rows. For now, leave everything as-is and click <strong>Next</strong>.</p>
<figure>
<img src="media/ads-external-table-csv-modify.png" title="Modify Columns" alt="The Modify Columns step is displayed." /><figcaption>The Modify Columns step is displayed.</figcaption>
</figure></li>
<li><p>Verify that everything looks correct in the Summary step, then click <strong>Create Table</strong>.</p>
<figure>
<img src="media/ads-external-table-csv-create.png" title="Summary" alt="The Summary step is displayed." /><figcaption>The Summary step is displayed.</figcaption>
</figure></li>
<li><p>As with the previous external table you created, a “Create External Table succeeded” dialog will appear under your task history in a few moments. Select the Servers link (Ctrl+G) on the left-hand menu, then expand the Tables list underneath your <strong>sales</strong> database and find the <strong>dbo.stockitemholdings (External)</strong> table. If you do not see it, right-click on the Tables folder, then select Refresh. <strong>Right-click</strong> the <strong>dbo.stockitemholdings (External)</strong> table, then select <strong>Select Top 1000</strong> from the context menu.</p>
<figure>
<img src="media/ads-stockitemholdings-select-top-1000.png" title="Select Top 1000" alt="The Select Top 1000 rows menu item is highlighted." /><figcaption>The Select Top 1000 rows menu item is highlighted.</figcaption>
</figure></li>
<li><p>Just as before, you should see a SQL query selecting the top 1000 rows and its query results, this time from the <code>stockitemholdings</code> table. Again, the SQL query is the same type of query you would write to select from a table internal to the sales database.</p>
<figure>
<img src="media/ads-stockitemholdings-results.png" title="Stockitemholdings results" alt="The stockitemholdings query and results are displayed." /><figcaption>The stockitemholdings query and results are displayed.</figcaption>
</figure>
<div class="sourceCode" id="cb2"><pre class="sourceCode sql"><code class="sourceCode sql"><a class="sourceLine" id="cb2-1" title="1"><span class="kw">SELECT</span> TOP (<span class="dv">1000</span>) [StockItemID]</a>
<a class="sourceLine" id="cb2-2" title="2">    ,[QuantityOnHand]</a>
<a class="sourceLine" id="cb2-3" title="3">    ,[BinLocation]</a>
<a class="sourceLine" id="cb2-4" title="4">    ,[LastStocktakeQuantity]</a>
<a class="sourceLine" id="cb2-5" title="5">    ,[LastCostPrice]</a>
<a class="sourceLine" id="cb2-6" title="6">    ,[ReorderLevel]</a>
<a class="sourceLine" id="cb2-7" title="7">    ,[TargetStockLevel]</a>
<a class="sourceLine" id="cb2-8" title="8">    ,[LastEditedBy]</a>
<a class="sourceLine" id="cb2-9" title="9">    ,[LastEditedWhen]</a>
<a class="sourceLine" id="cb2-10" title="10"><span class="kw">FROM</span> [sales_YOUR_UNIQUE_IDENTIFIER].[dbo].[stockitemholdings]</a></code></pre></div></li>
<li><p>Now that we have our two external tables added, we will now join those two external tables and two internal tables with a new SQL query to demonstrate how you can seamlessly combine all these data sources without having to copy any files or with separate queries or additional processing of that data. <strong>Right-click</strong> the <strong>sales_YOUR_UNIQUE_IDENTIFIER</strong> database, then select <strong>New Query</strong>.</p>
<figure>
<img src="media/ads-new-query.png" title="New Query" alt="The sales database and New Query menu item are highlighted." /><figcaption>The sales database and New Query menu item are highlighted.</figcaption>
</figure></li>
<li><p>Paste the following into the new query window:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode sql"><code class="sourceCode sql"><a class="sourceLine" id="cb3-1" title="1"><span class="kw">SELECT</span> i.i_item_sk <span class="kw">AS</span> ItemID, i.i_item_desc <span class="kw">AS</span> Item, c.c_first_name <span class="kw">AS</span> FirstName,</a>
<a class="sourceLine" id="cb3-2" title="2">  c.c_last_name <span class="kw">AS</span> LastName, s.QuantityOnHand, r.review <span class="kw">AS</span> Review, r.date_added <span class="kw">AS</span> DateReviewed</a>
<a class="sourceLine" id="cb3-3" title="3"><span class="kw">FROM</span> dbo.item <span class="kw">as</span> i</a>
<a class="sourceLine" id="cb3-4" title="4"><span class="kw">JOIN</span> dbo.Reviews <span class="kw">AS</span> r <span class="kw">ON</span> i.i_item_sk <span class="op">=</span> r.product_id</a>
<a class="sourceLine" id="cb3-5" title="5"><span class="kw">JOIN</span> dbo.customer <span class="kw">AS</span> c <span class="kw">ON</span> c.c_customer_sk <span class="op">=</span> r.customer_id</a>
<a class="sourceLine" id="cb3-6" title="6"><span class="kw">JOIN</span> dbo.stockitemholdings <span class="kw">AS</span> s <span class="kw">ON</span> i.i_item_sk <span class="op">=</span> s.StockItemID</a></code></pre></div></li>
<li><p>Click the <strong>Run</strong> button above the query window to execute.</p>
<figure>
<img src="media/ads-run.png" title="Run" alt="The Run button above the query window is highlighted." /><figcaption>The Run button above the query window is highlighted.</figcaption>
</figure></li>
<li><p>At the bottom of the query window, you will see results that include columns from the four data sources.</p>
<figure>
<img src="media/ads-query-results.png" title="Query results" alt="Query results from the four data sets." /><figcaption>Query results from the four data sets.</figcaption>
</figure></li>
</ol>
<h2 id="task-2-train-a-machine-learning-model-score-and-save-data-as-external-table">Task 2: Train a machine learning model, score and save data as external table</h2>
<p>In this task, you will use Azure Data Studio to execute a notebook that will enable you to train a model to predict the battery lifetime, apply the model to make batch predictions against a set of vehicle telemetry and save the scored telemetry to an external table that you can query using SQL.</p>
<ol type="1">
<li><p>Open Azure Data Studio and select Servers.</p></li>
<li><p>Right click your Big Data Cluster node select <code>Manage</code>.</p>
<figure>
<img src="media/task02-manage-cluster.png" title="Manage cluster" alt="Manage cluster" /><figcaption>Manage cluster</figcaption>
</figure></li>
<li><p>In the window, select the <code>SQL Big Data Cluster</code> tab and then select the <code>Open Notebook</code> tile.</p>
<figure>
<img src="media/task02-sql-bdc-manage.png" title="Open notebook" alt="Open notebook" /><figcaption>Open notebook</figcaption>
</figure></li>
<li><p>Browse to <strong>C:-files\1</strong>, select <strong>predict-battery-life-with-sqlbdc.ipynb</strong> and select <code>Open</code>.</p></li>
<li><p>Follow the instructions in the notebook and return to the next step after you have completed the notebook.</p>
<blockquote>
<p>There may be a kernel error pertaining to there not being a valid SQL connection when you open the notebook. If this happens, close the notebook and Azure Data Studio, then re-launch, reconnect, then re-open the notebook.</p>
</blockquote></li>
<li><p>In Azure Data Studio, under Servers, expand your Big Data Cluster, <code>Data Services</code>, <code>HDFS</code>, <code>data</code>.</p></li>
<li><p>Right click the <code>data</code> folder and select <code>Refresh</code> to see the newly created folder.</p>
<figure>
<img src="media/task02-refresh-data.png" title="Refresh data" alt="Refresh data" /><figcaption>Refresh data</figcaption>
</figure></li>
<li><p>You should see <code>battery-life-YOUR_UNIQUE_IDENTIFIER.csv</code> as a folder (where <code>YOUR_UNIQUE_IDENTIFIER</code> is your assigned identifier), expand it and then right click on the CSV file whose name starts with <code>part-00000-</code> and select <code>Create External Table From CSV Files</code>.</p>
<figure>
<img src="media/task02-create-external-menu.png" title="Create External Table" alt="Create External Table" /><figcaption>Create External Table</figcaption>
</figure></li>
<li><p>In Step 1 of the wizard, select your Active SQL Server connection to connect to your Big Data Cluster endpoint and select <code>Next</code>.</p>
<figure>
<img src="media/task02-ext-step1.png" title="Select endpoint" alt="Select endpoint" /><figcaption>Select endpoint</figcaption>
</figure></li>
<li><p>In Step 2, select the <code>sales_YOUR_UNIQUE_IDENTIFIER</code> database and for the <code>Name for new external table</code> field provide <code>battery-life-predictions</code>. Select Next.</p>
<figure>
<img src="media/task02-ext-step2.png" title="Step 2" alt="Step 2" /><figcaption>Step 2</figcaption>
</figure></li>
<li><p>On Step 3, select <code>Next</code>.</p></li>
<li><p>On Step 4, for the column <code>Car_Has_EcoStart</code> set the Data Type to <code>char(10)</code>. Select <code>Next</code>.</p>
<figure>
<img src="media/task02-ext-step4.png" title="Step 4" alt="Step 4" /><figcaption>Step 4</figcaption>
</figure></li>
<li><p>On Step 5, select <code>Create Table</code>. Your predictions are now available for SQL querying in the battery-life-predictions table in the sales database.</p></li>
<li><p>In Azure Data Studio, Servers, expand your Big Data Cluster, <code>Databases</code>, <code>sales_YOUR-UNIQUE-IDENTIFIER</code>, right click <code>Tables</code> and then select <code>Refresh</code>.</p>
<figure>
<img src="media/task02-refresh-sales.png" title="Refresh sales" alt="Refresh sales" /><figcaption>Refresh sales</figcaption>
</figure></li>
<li><p>Expand <code>tables</code>, right click <code>battery-life-prediction</code> and select <code>query</code> to view the data contained by the external table.</p>
<figure>
<img src="media/task02-select-top.png" title="Select Top 1000" alt="Select Top 1000" /><figcaption>Select Top 1000</figcaption>
</figure></li>
<li><p>The vehicle telemetry along with predictions will appear. These are queried from the external table which is sourced from the CSV you created using the notebook.</p>
<figure>
<img src="media/task02-view-data.png" title="View data" alt="View data" /><figcaption>View data</figcaption>
</figure></li>
</ol>
<h2 id="task-3-identify-pii-and-gdpr-related-compliance-issues-using-data-discovery-classification-in-ssms">Task 3: Identify PII and GDPR-related compliance issues using Data Discovery &amp; Classification in SSMS</h2>
<p>Contoso Auto has several databases that include tables containing sensitive data, such as personally identifiable information (PII) like phone numbers, social security numbers, financial data, etc. Since some of their personnel and customer data include individuals who reside within the European Union (EU), they need to adhere to the General Data Protection Regulation (<a href="https://en.wikipedia.org/wiki/General_Data_Protection_Regulation">GDPR</a>) as well. Because of this, Contoso Auto is required to provide periodic data auditing reports to identify sensitive and GDPR-related data that reside within their various databases.</p>
<p>With SQL Server Management Studio, they are able to identify, classify, and generate reports on sensitive and GDPR-related data by using the <a href="https://docs.microsoft.com/sql/relational-databases/security/sql-data-discovery-and-classification?view=sql-server-ver15">SQL Data Discovery &amp; Classification</a> tool. This tool introduces a set of advanced services, forming a new SQL Information Protection paradigm aimed at protecting the data, not just the database:</p>
<ul>
<li><strong>Discovery &amp; recommendations</strong> - The classification engine scans your database and identifies columns containing potentially sensitive data. It then provides you an easy way to review and apply the appropriate classification recommendations, as well as to manually classify columns.</li>
<li><strong>Labeling</strong> - Sensitivity classification labels can be persistently tagged on columns.</li>
<li><strong>Visibility</strong> - The database classification state can be viewed in a detailed report that can be printed/exported to be used for compliance &amp; auditing purposes, as well as other needs.</li>
</ul>
<p>In this exercise, you will run the SQL Data Discovery &amp; Classification tool against their customer database, which includes personal, demographic, and sales data.</p>
<ol type="1">
<li><p>Open SQL Server Management Studio (SSMS) and connect to your SQL Server 2019 cluster.</p></li>
<li><p>Right-click on the <strong>sales_YOUR_UNIQUE_IDENTIFIER</strong> database, then choose <strong>Tasks &gt; Classify Data…</strong>.</p>
<figure>
<img src="media/ssms-classify-data-link.png" title="Data Classification" alt="The sales database, Tasks menu, and Classify Data items are highlighted." /><figcaption>The sales database, Tasks menu, and Classify Data items are highlighted.</figcaption>
</figure></li>
<li><p>When the tool runs, it will analyze all of the columns within all of the tables and recommend appropriate data classifications for each. What you should see is the Data Classification dashboard showing no currently classified columns, and a classification recommendations box at the top showing that there are 45 columns that the tool identified as containing sensitive (PII) or GDPR-related data. <strong>Click</strong> on this classification recommendations box.</p>
<figure>
<img src="media/ssms-classification-recommendations-box.png" title="Data classification recommendations box" alt="The data classification recommendations box is highlighted." /><figcaption>The data classification recommendations box is highlighted.</figcaption>
</figure></li>
<li><p>The list of recommendations displays the schema, table, column, type of information, and recommended sensitivity label for each identified column. You can change the information type and sensitivity labels for each if desired. In this case, accept all recommendations by <strong>checking the checkbox</strong> in the recommendations table header.</p>
<figure>
<img src="media/ssms-recommendations.png" title="Classification recommendations" alt="The recommendations are shown with each checkbox checked." /><figcaption>The recommendations are shown with each checkbox checked.</figcaption>
</figure></li>
<li><p>Click <strong>Accept selected recommendations</strong>.</p>
<figure>
<img src="media/ssms-accept-selected-recommendations.png" title="Accept selected recommendations" alt="The Accept selected recommendations button is highlighted." /><figcaption>The Accept selected recommendations button is highlighted.</figcaption>
</figure></li>
<li><p>Click <strong>Save</strong> in the toolbar above to apply your changes.</p>
<figure>
<img src="media/ssms-save-classification-changes.png" title="Save classification changes" alt="The Save button is highlighted." /><figcaption>The Save button is highlighted.</figcaption>
</figure></li>
<li><p>After the changes are saved, click <strong>View Report</strong>.</p>
<figure>
<img src="media/ssms-view-report.png" title="View Report" alt="The View Report button is highlighted." /><figcaption>The View Report button is highlighted.</figcaption>
</figure></li>
<li><p>What you should see is a report with a full summary of the database classification state. When you right-click on the report, you can see options to print or export the report in different formats.</p>
<figure>
<img src="media/ssms-report.png" title="SQL Data Classification Report" alt="The report is displayed, as well as the context menu showing export options after right-clicking on the report." /><figcaption>The report is displayed, as well as the context menu showing export options after right-clicking on the report.</figcaption>
</figure></li>
</ol>
<h2 id="task-4-fix-compliance-issues-with-dynamic-data-masking">Task 4: Fix compliance issues with dynamic data masking</h2>
<p>Some of the columns identified by the Data Discovery &amp; Classification tool as containing sensitive (PII/GDPR) information include phone numbers, email addresses, billing addresses, and credit card numbers. One way to ensure compliance with various rules and regulations that enforce policies to protect such sensitive data is to prevent those who are not authorized from seeing it. An example would be displaying <code>XXX-XXX-XX95</code> instead of <code>123-555-2695</code> when outputting a phone number within a SQL query result, report, web page, etc. This is commonly called data masking. Traditionally, modifying systems and applications to implement data masking can be challenging. This is especially true when the masking has to apply all the way down to the data source level. Fortunately, SQL Server and its cloud-related product, Azure SQL Database, provides a feature named <a href="https://docs.microsoft.com/sql/relational-databases/security/dynamic-data-masking?view=sql-server-ver15">dynamic data masking</a> (DDM) to automatically protect this sensitive data from non-privileged users.</p>
<p>Dynamic data masking helps prevent unauthorized access to sensitive data by enabling customers to designate how much of the sensitive data to reveal with minimal impact on the application layer. DDM can be configured on the database to hide sensitive data in the result sets of queries over designated database fields, while the data in the database is not changed. Dynamic data masking is easy to use with existing applications, since masking rules are applied in the query results. Many applications can mask sensitive data without modifying existing queries.</p>
<p>In this task, you will apply dynamic data masking to one of the database fields so you can see how to address the reported compliance issues. To test the data mask, you will create a test user and query the field as that user.</p>
<ol type="1">
<li><p>Open SQL Server Management Studio (SSMS) and connect to your SQL Server 2019 cluster.</p></li>
<li><p>Expand the databases list, right-click on <strong>sales_YOUR_UNIQUE_IDENTIFIER</strong>, then select <strong>New Query</strong>.</p>
<figure>
<img src="media/ssms-sales-new-query.png" title="New Query" alt="The sales database and New Query menu item are highlighted." /><figcaption>The sales database and New Query menu item are highlighted.</figcaption>
</figure></li>
<li><p>Add a dynamic data mask to the existing <code>dbo.customer.c_last_name</code> field by pasting the below query into the new query window:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode sql"><code class="sourceCode sql"><a class="sourceLine" id="cb4-1" title="1"><span class="kw">ALTER</span> <span class="kw">TABLE</span> dbo.customer</a>
<a class="sourceLine" id="cb4-2" title="2"><span class="kw">ALTER</span> <span class="kw">COLUMN</span> c_last_name <span class="kw">ADD</span> MASKED <span class="kw">WITH</span> (<span class="kw">FUNCTION</span> <span class="op">=</span> <span class="st">&#39;partial(2,&quot;XXX&quot;,0)&#39;</span>);</a></code></pre></div>
<blockquote>
<p>The <code>partial</code> custom string masking method above exposes the first two characters and adds a custom padding string after for the remaining characters. The parameters are: <code>prefix,[padding],suffix</code></p>
</blockquote></li>
<li><p>Execute the query by clicking the <strong>Execute</strong> button above the query window, or enter <em>F5</em>.</p>
<figure>
<img src="media/ssms-execute-ddm-query.png" title="Execute query" alt="The dynamic data mask query is shown and the Execute button is highlighted above." /><figcaption>The dynamic data mask query is shown and the Execute button is highlighted above.</figcaption>
</figure></li>
<li><p>Clear the query window and replace the previous query with the following to add a dynamic data mask to the <code>dbo.customer.c_email_address</code> field:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode sql"><code class="sourceCode sql"><a class="sourceLine" id="cb5-1" title="1"><span class="kw">ALTER</span> <span class="kw">TABLE</span> dbo.customer</a>
<a class="sourceLine" id="cb5-2" title="2"><span class="kw">ALTER</span> <span class="kw">COLUMN</span> c_email_address <span class="kw">ADD</span> MASKED <span class="kw">WITH</span> (<span class="kw">FUNCTION</span> <span class="op">=</span> <span class="st">&#39;email()&#39;</span>);</a></code></pre></div>
<blockquote>
<p>The <code>email</code> masking method exposes the first letter of an email address and the constant suffix “.com”, in the form of an email address: <code>aXXX@XXXX.com</code>.</p>
</blockquote></li>
<li><p>Clear the query window and replace the previous query with the following, selecting all rows from the customer table:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode sql"><code class="sourceCode sql"><a class="sourceLine" id="cb6-1" title="1"><span class="kw">SELECT</span> <span class="op">*</span> <span class="kw">FROM</span> dbo.customer</a></code></pre></div>
<figure>
<img src="media/ssms-ddm-results-no-mask.png" title="Query results" alt="The query results are shown with no mask applied to the Postal Code field." /><figcaption>The query results are shown with no mask applied to the Postal Code field.</figcaption>
</figure></li>
<li><p>Notice that the full last name and email address values are visible. That is because the user you are logged in as a privileged user. Let’s create a new user and execute the query again:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode sql"><code class="sourceCode sql"><a class="sourceLine" id="cb7-1" title="1"><span class="kw">CREATE</span> <span class="fu">USER</span> TestUser <span class="kw">WITHOUT</span> LOGIN;</a>
<a class="sourceLine" id="cb7-2" title="2"><span class="kw">GRANT</span> <span class="kw">SELECT</span> <span class="kw">ON</span> dbo.customer <span class="kw">TO</span> TestUser;</a>
<a class="sourceLine" id="cb7-3" title="3"></a>
<a class="sourceLine" id="cb7-4" title="4"><span class="kw">EXECUTE</span> <span class="kw">AS</span> <span class="fu">USER</span> <span class="op">=</span> <span class="st">&#39;TestUser&#39;</span>;</a>
<a class="sourceLine" id="cb7-5" title="5"><span class="kw">SELECT</span> <span class="op">*</span> <span class="kw">FROM</span> dbo.customer;</a>
<a class="sourceLine" id="cb7-6" title="6">REVERT;</a></code></pre></div></li>
<li><p>Execute the query by clicking the <strong>Execute</strong> button. Notice this time that the Postal Code values are masked (<code>90XXX</code>).</p>
<figure>
<img src="media/ssms-ddm-results-mask.png" title="Query results" alt="The query results are shown with the mask applied to the Postal Code field." /><figcaption>The query results are shown with the mask applied to the Postal Code field.</figcaption>
</figure></li>
</ol>
<h2 id="wrap-up">Wrap-up</h2>
<p>Thank you for participating in the SQL Server 2019 Big Data Clusters experience! We hope you are excited about the new capabilities, and will refer back to this experience to learn more about these features.</p>
<p>To recap, you experienced:</p>
<ol type="1">
<li>How to minimize or remove the need for ETL through <strong>data virtualization</strong> with <a href="https://docs.microsoft.com/sql/relational-databases/polybase/data-virtualization?toc=%2fsql%2fbig-data-cluster%2ftoc.json&amp;view=sql-server-ver15">relational data sources</a> and <a href="https://docs.microsoft.com/sql/relational-databases/polybase/data-virtualization-csv?view=sql-server-ver15">CSV files</a>, by being able to query against these alongside internal SQL 2019 tables with no data movement required.</li>
<li>Training a machine learning model by running a Jupyter notebook on the Big Data cluster, then scoring data with the trained model and saving it as an external table for easy access.</li>
<li>Using the <a href="https://docs.microsoft.com/sql/relational-databases/security/sql-data-discovery-and-classification?view=sql-server-ver15">SQL Data Discovery &amp; Classification</a> tool to identify and tag PII and GDPR-related compliance issues.</li>
<li>Used dynamic data masking to automatically protect sensitive data from unauthorized users.</li>
</ol>
<h2 id="additional-resources-and-more-information">Additional resources and more information</h2>
<ul>
<li><a href="https://docs.microsoft.com/en-us/sql/sql-server/what-s-new-in-sql-server-ver15?view=sql-server-ver15">What’s new in SQL Server 2019 preview</a></li>
<li><a href="https://docs.microsoft.com/en-us/sql/big-data-cluster/big-data-cluster-overview?view=sql-server-ver15">SQL Server 2019 big data clusters overview and architecture</a></li>
<li><a href="https://docs.microsoft.com/en-us/sql/big-data-cluster/tutorial-notebook-spark?view=sqlallproducts-allversions">How to run a sample notebook in Azure Data Studio on a SQL Server 2019 big data cluster, and leverage Spark</a></li>
<li><a href="https://docs.microsoft.com/en-us/sql/azure-data-studio/what-is?view=sql-server-ver15">What is Azure Data Studio?</a></li>
<li><a href="https://docs.microsoft.com/en-us/sql/relational-databases/security/security-center-for-sql-server-database-engine-and-azure-sql-database?view=sql-server-2017">Security Center for SQL Server Database Engine and Azure SQL Database</a></li>
<li><a href="https://docs.microsoft.com/en-us/sql/relational-databases/security/sql-data-discovery-and-classification?view=sql-server-2017">SQL Data Discovery and Classification tool documentation</a></li>
<li><a href="https://docs.microsoft.com/en-us/sql/relational-databases/performance/intelligent-query-processing?view=sql-server-2017">Intelligent query processing in SQL databases</a></li>
<li><a href="https://docs.microsoft.com/en-us/sql/advanced-analytics/what-s-new-in-sql-server-machine-learning-services?view=sql-server-ver15">What’s new in SQL Server Machine Learning Services</a></li>
<li><a href="https://docs.microsoft.com/en-us/sql/advanced-analytics/java/extension-java?view=sql-server-ver15">How to run Java code in SQL Server 2019</a></li>
<li><a href="https://github.com/Microsoft/sqlworkshops">Learning content in GitHub: SQL Server Workshops</a></li>
<li><a href="https://github.com/Microsoft/sql-server-samples">SQL Server Samples Repository in GitHub. Feature demos, code samples etc.</a></li>
</ul>
<p>SQL Server 2019 has a new database compatibility level: <code>150</code>. When you set your database to this level, you will gain query performance benefits due to improvements to the family of intelligent query processing (QP) features in SQL Server 2019. These features improve the performance of existing workloads with minimal work on your part to implement.</p>
<p>A few QP features we would like to call out are:</p>
<ol type="1">
<li><a href="https://docs.microsoft.com/sql/relational-databases/user-defined-functions/scalar-udf-inlining?view=sql-server-2017">Scalar UDF inlining</a>: This feature automatically transforms <a href="https://docs.microsoft.com/sql/relational-databases/user-defined-functions/create-user-defined-functions-database-engine?view=sql-server-2017#Scalar">scalar UDFs</a> into relational expressions. It embeds them in the calling SQL query. This transformation improves the performance of workloads that take advantage of scalar UDFs. Scalar UDF inlining facilitates cost-based optimization of operations inside UDFs. The results are efficient, set-oriented, and parallel instead of inefficient, iterative, serial execution plans. This feature is enabled by default under database compatibility level 150.</li>
<li><a href="https://docs.microsoft.com/sql/t-sql/data-types/table-transact-sql?view=sql-server-2017#table-variable-deferred-compilation">Table variable deferred compilation</a>: This feature improves plan quality and overall performance for queries that reference table variables. During optimization and initial compilation, this feature propagates cardinality estimates that are based on actual table variable row counts. This accurate row count information optimizes downstream plan operations. Table variable deferred compilation defers compilation of a statement that references a table variable until the first actual run of the statement. This deferred compilation behavior is the same as that of temporary tables. This change results in the use of actual cardinality instead of the original one-row guess.</li>
<li><a href="https://docs.microsoft.com/sql/relational-databases/performance/adaptive-query-processing?view=sql-server-2017#row-mode-memory-grant-feedback">Row mode memory grant feedback</a>: A query’s post-execution plan in SQL Server includes the minimum required memory needed for execution and the ideal memory grant size to have all rows fit in memory. Performance suffers when memory grant sizes are incorrectly sized. Excessive grants result in wasted memory and reduced concurrency. Insufficient memory grants cause expensive spills to disk. By addressing repeating workloads, batch mode memory grant feedback recalculates the actual memory required for a query and then updates the grant value for the cached plan. <strong>When an identical query statement is executed</strong>, the query uses the revised memory grant size, reducing excessive memory grants that impact concurrency and fixing underestimated memory grants that cause expensive spills to disk. Row mode memory grant feedback expands on the batch mode memory grant feedback feature by adjusting memory grant sizes for both batch and row mode operators.</li>
</ol>
<p>Read more about <a href="https://docs.microsoft.com/sql/relational-databases/performance/intelligent-query-processing?view=sql-server-ver15">intelligent query processing</a> in SQL databases.</p>
